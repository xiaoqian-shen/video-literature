# Awesome Papers on Video/3D Generation and Representation 

## Video Generation

#### 2023

PVDM: Video Probabilistic Diffusion Models in Projected Latent Space

[[code]](https://github.com/sihyun-yu/PVDM) (CVPR 2023)

MAGVIT: Masked Generative Video Transformer

[[paper]](https://magvit.cs.cmu.edu/)[[page]]()[[code(coming soon)]](https://github.com/MAGVIT/magvit) 

MagicVideo: Efficient Video Generation With Latent Diffusion Models

[[paper]](https://arxiv.org/abs/2211.11018)[[page]](https://magicvideo.github.io/#)

Phenaki: Variable Length Video Generation From Open Domain Textual Description

[[paper]](https://arxiv.org/pdf/2210.02399.pdf) (ICLR 2023)

Make-A-Video: Text-to-Video Generation without Text-Video Data

(ICLR 2023)

StyleFaceV: Face Video Generation via Decomposing and Recomposing Pretrained StyleGAN3

[[paper]](https://arxiv.org/abs/2208.07862)[[page]](http://haonanqiu.com/projects/StyleFaceV.html)[[code]](https://github.com/arthur-qiu/StyleFaceV)

CogVideo: Large-scale Pretraining for Text-to-Video Generation via Transformers 

[[paper]](https://arxiv.org/pdf/2205.15868.pdf)[[page]](https://models.aminer.cn/cogvideo)[[code]](https://github.com/THUDM/CogVideo) (ICLR 2023)

#### 2022

Generating Long Videos of Dynamic Scenes

[][paper][[page]](http://arxiv.org/abs/2206.03429)[[page]](https://www.timothybrooks.com/tech/long-videos/)[[code]](https://github.com/NVlabs/long-video-gan) (NeurIPS 2022)

Video Diffusion Models

[[paper]](http://arxiv.org/abs/2204.03458)[[page]](https://video-diffusion.github.io/) (NeurIPS 2022)

MCVD: Masked Conditional Video Diffusion for Prediction, Generation, and Interpolation

[[paper]](https://arxiv.org/abs/2205.09853)[[page]](https://mask-cond-video-diffusion.github.io/)[[code]](https://github.com/voletiv/mcvd-pytorch) (NeurIPS 2022)

TATS: Long Video Generation with Time-Agnostic VQGAN and Time-Sensitive Transformer

[[paper]](http://arxiv.org/abs/2204.03638)[[code]](https://github.com/SongweiGe/TATS) (ECCV 2022)

CelebV-HQ: A Large-Scale Video Facial Attributes Dataset

[[paper]](https://arxiv.org/abs/2207.12393)[[page]](https://celebv-hq.github.io/) (ECCV 2022)

DIGAN: Generating Videos with Dynamics-aware Implicit Generative Adversarial Networks

[[paper]](http://arxiv.org/abs/2202.10571)[[code]]((https://github.com/sihyun-yu/digan)) (ICLR 2022)

MUGEN: A Playground for Video-Audio-Text Multimodal Understanding and GENeration

[[paper]](https://arxiv.org/abs/2204.08058v3)[[page]](https://mugen-org.github.io/)[[code]](https://github.com/mugen-org/MUGEN_baseline) (ECCV 2022)

VideoINR: Learning Video Implicit Neural Representation for Continuous Space-Time Super-Resolution

[[paper]](https://arxiv.org/abs/2206.04647)[[code]](https://github.com/Picsart-AI-Research/VideoINR-Continuous-Space-Time-Super-Resolution) (CVPR 2022)

Show Me What and Tell Me How: Video Synthesis via Multimodal Conditioning

[[paper]](http://arxiv.org/abs/2203.02573)[[page]](https://snap-research.github.io/MMVID/)[[code]](https://github.com/snap-research/MMVID) (CVPR 2022)

StyleGAN-V: A Continuous Video Generator with the Price, Image Quality and Perks of StyleGAN2

[[paper]](https://arxiv.org/abs/2112.14683)[[page]](https://universome.github.io/stylegan-v)[[code]](https://github.com/universome/stylegan-v) (CVPR 2022)

Video2StyleGAN: Disentangling Local and Global Variations in a Video

[[paper]](https://arxiv.org/abs/2205.13996v2)

#### 2021

CCVS: Context-aware Controllable Video Synthesis

[[paper]](http://arxiv.org/abs/2107.08037) NeurIPS

V3GAN: Decomposing Background, Foreground and Motion for Video Generation

[[paper]](https://arxiv.org/abs/2203.14074v1)

Playable Video Generation

[[paper]](https://arxiv.org/abs/2101.12195v1)[[code]](https://github.com/willi-menapace/PlayableVideoGeneration) (CVPR 2021 Oral)

Stochastic Image-to-Video Synthesis using cINNs

[[paper]](https://arxiv.org/abs/2105.04551)[[page]](https://compvis.github.io/image2video-synthesis-using-cINNs/)[[code]](https://github.com/CompVis/image2video-synthesis-using-cINNs) (CVPR 2021)

Generative Video Transformer: Can Objects be the Words?

[[paper]](http://arxiv.org/abs/2107.09240) (ICML 2021)

VideoGPT: Video Generation using VQ-VAE and Transformers

[[paper]]((https://arxiv.org/abs/2104.10157))[[code]](https://github.com/wilson1yan/VideoGPT)

StyleVideoGAN: A Temporal Generative Model using a Pretrained StyleGAN

[[paper]](https://arxiv.org/abs/2107.07224v2) 

MoCoGAN-HD: A Good Image Generator Is What You Need for High-Resolution Video Synthesis

[[paper]](https://arxiv.org/abs/2104.15069v1)[[code]](https://github.com/snap-research/MoCoGAN-HD) (ICLR 2021 Spotlight)

InMoDeGAN: Interpretable Motion Decomposition Generative Adversarial Network for Video Generation

[[paper]](https://arxiv.org/abs/2101.03049v1)[[page]](https://wyhsirius.github.io/InMoDeGAN/)

Temporal Shift GAN for Large Scale Video Generation

[[paper]](https://openaccess.thecvf.com/content/WACV2021/papers/Munoz_Temporal_Shift_GAN_for_Large_Scale_Video_Generation_WACV_2021_paper.pdf) (WACV 2021)

#### 2020

Infinite Nature: Perpetual View Generation of Natural Scenes from a Single Image

[[paper]](https://arxiv.org/abs/2012.09855v4)[[page]](https://infinite-nature.github.io/)[[code]](https://github.com/google-research/google-research/tree/master/infinite_nature) (ICCV 2021 oral)

Learning Temporal Coherence via Self-Supervision for GAN-based Video Generation

[[paper]](http://arxiv.org/abs/1811.09393)[[page]](https://ge.in.tum.de/publications/2019-tecogan-chu/)[[code]](https://github.com/thunil/TecoGAN) (ACM Graphics)

G3AN: Disentangling Appearance and Motion for Video Generation

[[paper]](https://ieeexplore.ieee.org/document/9157816/)[[code]](https://github.com/wyhsirius/g3an-project) (CVPR 2020)

ImaGINator: Conditional Spatio-Temporal GAN for Video Generation

[[paper]](https://ieeexplore.ieee.org/document/9093492/) (WACV)

2019

Conditional GAN with Discriminative Filter Generation for Text-to-Video Synthesis

[[paper]](https://www.ijcai.org/proceedings/2019/276) IJCAI

#### 2018

MoCoGAN: Decomposing Motion and Content for Video Generation

[[paper]](https://ieeexplore.ieee.org/document/8578263/) CVPR

#### 2017

TGAN: Temporal Generative Adversarial Nets with Singular Value Clipping

[[paper]](http://ieeexplore.ieee.org/document/8237570/) ICCV

#### 2016

Generating Videos with Scene Dynamics

[[paper]](https://proceedings.neurips.cc/paper/2016/hash/04025959b191f8f9de3f924f0940515f-Abstract.html) NeurIPS

### Conditional

Temporally Consistent Semantic Video Editing

[[paper]](https://arxiv.org/abs/2206.10590v1)

## Video Representation

#### 2022

Scalable Neural Video Representations with Learnable Positional Features

[[paper]](https://arxiv.org/abs/2210.06823)[[page]](https://subin-kim-cv.github.io/NVP/)[[code]](https://github.com/subin-kim-cv/NVP) (NeurIPS 2022)

MCL: Motion-Focused Contrastive Learning of Video Representations

[[paper]](https://arxiv.org/abs/2201.04029v1) (ICCV 2022 oral)

#### 2021

FAME: Motion-aware Contrastive Video Representation Learning via Foreground-background Merging

[[paper]](https://arxiv.org/abs/2109.15130v3) CVPR

TAM: Temporal Adaptive Module for Video Recognition

[[paper]](http://arxiv.org/abs/2005.06803) ICCV

Self-supervised Video Representation Learning by Context and Motion Decoupling

[[paper]](https://arxiv.org/abs/2104.00862v1) CVPR

Enhancing Unsupervised Video Representation Learning by Decoupling the Scene and the Motion

[[paper]](https://arxiv.org/abs/2009.05757v3) AAAI

## Others

#### 2022

3D-Aware Video Generation

[[paper]](https://arxiv.org/abs/2206.14797v1)

Latent Image Animator: Learning to Animate Images via Latent Space Navigation

[[paper]](https://arxiv.org/abs/2203.09043v1)[[code]](https://github.com/wyhsirius/LIA) ICLR

`source to target, find latent direction`

Stochastic Backpropagation: A Memory Efficient Strategy for Training Video Models

[[paper]](https://arxiv.org/abs/2203.16755v1) CVPR

`bottom layer has redundancy, randomly drop gradients of spatial model`